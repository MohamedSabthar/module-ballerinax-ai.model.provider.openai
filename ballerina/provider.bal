// Copyright (c) 2025 WSO2 LLC (http://www.wso2.com).
//
// WSO2 LLC. licenses this file to you under the Apache License,
// Version 2.0 (the "License"); you may not use this file except
// in compliance with the License.
// You may obtain a copy of the License at
//
// http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing,
// software distributed under the License is distributed on an
// "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
// KIND, either express or implied.  See the License for the
// specific language governing permissions and limitations
// under the License.

import ballerina/ai;
import ballerinax/openai.chat;

const DEFAULT_OPENAI_SERVICE_URL = "https://api.openai.com/v1";
const DEFAULT_MAX_TOKEN_COUNT = 512;
const DEFAULT_TEMPERATURE = 0.7d;

# Provider is a client class that provides an interface for interacting with OpenAI Large Language Models.
public isolated client class Provider {
    *ai:ModelProvider;
    private final chat:Client llmClient;
    private final OPEN_AI_MODEL_NAMES modelType;

    # Initializes the OpenAI model with the given connection configuration and model configuration.
    #
    # + apiKey - The OpenAI API key
    # + modelType - The OpenAI model name
    # + serviceUrl - The base URL of OpenAI API endpoint
    # + maxTokens - The upper limit for the number of tokens in the response generated by the model
    # + temperature - The temperature for controlling randomness in the model's output  
    # + connectionConfig - Additional HTTP connection configuration
    # + return - `()` on successful initialization; otherwise, returns an `ai:Error`
    public isolated function init(@display {label: "API Key"} string apiKey,
            @display {label: "Model Type"} OPEN_AI_MODEL_NAMES modelType,
            @display {label: "Service URL"} string serviceUrl = DEFAULT_OPENAI_SERVICE_URL,
            @display {label: "Maximum Tokens"} int maxTokens = DEFAULT_MAX_TOKEN_COUNT,
            @display {label: "Temperature"} decimal temperature = DEFAULT_TEMPERATURE,
            @display {label: "Connection Configuration"} *ConnectionConfig connectionConfig) returns ai:Error? {
        chat:ClientHttp1Settings?|error http1Settings = connectionConfig?.http1Settings.cloneWithType();
        if http1Settings is error {
            return error ai:Error("Failed to clone http1Settings", http1Settings);
        }
        chat:ConnectionConfig openAiConfig = {
            auth: {
                token: apiKey
            },
            httpVersion: connectionConfig.httpVersion,
            http1Settings: http1Settings,
            http2Settings: connectionConfig.http2Settings,
            timeout: connectionConfig.timeout,
            forwarded: connectionConfig.forwarded,
            poolConfig: connectionConfig.poolConfig,
            cache: connectionConfig.cache,
            compression: connectionConfig.compression,
            circuitBreaker: connectionConfig.circuitBreaker,
            retryConfig: connectionConfig.retryConfig,
            responseLimits: connectionConfig.responseLimits,
            secureSocket: connectionConfig.secureSocket,
            proxy: connectionConfig.proxy,
            validation: connectionConfig.validation
        };
        chat:Client|error llmClient = new (openAiConfig, serviceUrl);
        if llmClient is error {
            return error ai:Error("Failed to initialize OpenAiProvider", llmClient);
        }
        self.llmClient = llmClient;
        self.modelType = modelType;
    }

    # Sends a chat request to the OpenAI model with the given messages and tools.
    #
    # + messages - List of chat messages 
    # + tools - Tool definitions to be used for the tool call
    # + stop - Stop sequence to stop the completion
    # + return - Function to be called, chat response or an error in-case of failures
    isolated remote function chat(ai:ChatMessage[] messages, ai:ChatCompletionFunctions[] tools, string? stop = ())
        returns ai:ChatAssistantMessage|ai:LlmError {
        chat:CreateChatCompletionRequest request = {
            stop,
            model: self.modelType,
            messages: self.mapToChatCompletionRequestMessage(messages, tools)
        };
        boolean canCallTools = isToolCallSupported(self.modelType);
        if canCallTools && tools.length() > 0 {
            request.functions = tools;
        }
        chat:CreateChatCompletionResponse|error response = self.llmClient->/chat/completions.post(request);
        if response is error {
            return error ai:LlmConnectionError("Error while connecting to the model", response);
        }
        chat:CreateChatCompletionResponse_choices[] choices = response.choices;
        if choices.length() == 0 {
            return error ai:LlmInvalidResponseError("Empty response from the model when using function call API");
        }
        return self.mapToChatAssistantMessage(choices[0].message);
    }

    private isolated function mapToChatCompletionRequestMessage(ai:ChatMessage[] messages, ai:ChatCompletionFunctions[] tools)
        returns chat:ChatCompletionRequestMessage[] {
        chat:ChatCompletionRequestMessage[] chatCompletionRequestMessages = [];
        boolean canCallTools = isToolCallSupported(self.modelType);
        foreach ai:ChatMessage message in messages {
            if message is ai:ChatSystemMessage && !canCallTools {
                string reactPrompt = constructReActPrompt(extractToolInfo(tools), message.content);
                chatCompletionRequestMessages.push({role: ai:SYSTEM, content: reactPrompt});
            } else if message is ai:ChatAssistantMessage {
                chat:ChatCompletionRequestAssistantMessage assistantMessage = {role: ai:ASSISTANT};
                ai:FunctionCall[]? toolCalls = message.toolCalls;
                if canCallTools && toolCalls is ai:FunctionCall[] {
                    assistantMessage.function_call = toolCalls[0];
                } else if !canCallTools && toolCalls is ai:FunctionCall[] {
                    // assistantMessage.content = toolCalls[0].toJsonString();
                    assistantMessage = mapFunctionCallToJsonBlob(toolCalls[0]);
                }
                string? content = message?.content;
                if canCallTools && content is string {
                    assistantMessage.content = content;
                } else if !canCallTools && content is string {
                    assistantMessage = mapToFinalAnswer(content);
                }
                chatCompletionRequestMessages.push(assistantMessage);
            } else {
                chatCompletionRequestMessages.push(message);
            }
        }
        return chatCompletionRequestMessages;
    }

    private isolated function mapToChatAssistantMessage(chat:ChatCompletionResponseMessage? message) returns ai:ChatAssistantMessage|ai:LlmError {
        boolean hasToolCallResponse = isToolCallSupported(self.modelType);
        ai:ChatAssistantMessage chatAssistantMessage = {role: ai:ASSISTANT};
        if hasToolCallResponse {
            chatAssistantMessage.content = message?.content;
            chat:ChatCompletionRequestAssistantMessage_function_call? functionCall = message?.function_call;
            if functionCall is chat:ChatCompletionRequestAssistantMessage_function_call {
                chatAssistantMessage.toolCalls = [
                    {
                        name: functionCall.name,
                        arguments: functionCall.arguments
                    }
                ];
            }
            return chatAssistantMessage;
        }
        ai:LlmToolResponse|ai:LlmChatResponse parsedReActResponse = check parseReActLlmResponse(message?.content);
        if parsedReActResponse is ai:LlmToolResponse {
            chatAssistantMessage.toolCalls = [
                {
                    name: parsedReActResponse.name,
                    arguments: parsedReActResponse.arguments.toJsonString()
                }
            ];
        } else {
            // Set the "Final Answer" action's input to the chat assistant message content
            chatAssistantMessage.content = parsedReActResponse.content;
        }
        return chatAssistantMessage;
    }
}

isolated function mapFunctionCallToJsonBlob(ai:FunctionCall toolCall) returns chat:ChatCompletionRequestAssistantMessage {
    return {
        role: "assistant",
        content: string `${BACKTICKS}json
{
    "${ACTION_KEY}": "${toolCall.name}",
    "${ACTION_INPUT_KEY}": ${toolCall.arguments.toJsonString()}
}
${BACKTICKS}`
    };
}

isolated function mapToFinalAnswer(string input) returns chat:ChatCompletionRequestAssistantMessage {
    return {
        role: "assistant",
        content: string `${BACKTICKS}json
{
    "${ACTION_KEY}": "${FINAL_ANSWER_KEY}",
    "${ACTION_INPUT_KEY}": "${input}"
}
${BACKTICKS}`
    };
}
